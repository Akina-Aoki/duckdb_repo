# Workflow: Loading Sakila from SQLite into DuckDB Using DLT

This end-to-end workflow provisions a local DuckDB warehouse from a SQLite source using dlt.
All steps below assume the working directory:
```bash
~/de25/aira_sql/video_code_along/19_dlt (main)
```

## Initialization
| Step                        | Command / Action                                 | Purpose                                      |
| --------------------------- | ------------------------------------------------ | -------------------------------------------- |
| 1. Create project structure | `uv init`                                        | Generates `pyproject.toml` and scaffolding   |
| 2. Rename project           | Update `name = "dlt_sakila"` in `pyproject.toml` | Standardizes package metadata                |
| 3. Add dependencies         | `uv add ipykernel "dlt[sql_database]"`           | Enables database extraction and notebook use |


## Preparing Data Assets
| File               | Location                       | Notes                                         |
| ------------------ | ------------------------------ | --------------------------------------------- |
| `sqlite-sakila.db` | `19_dlt/data/`                 | Downloaded from Kaggle; **must not be empty** |
| `sakila.duckdb`    | Auto-created in `19_dlt/data/` | Generated by DLT pipeline                     |


## Pipeline Script to write in `load_sakila_sqlite_duckdb.py`
```python
import dlt
from dlt.sources.sql_database import sql_database
from pathlib import Path

DATA_PATH = Path(__file__).parent / "data"
SQLITE_PATH = DATA_PATH / "sqlite-sakila.db"
DUCKDB_PATH = DATA_PATH / "sakila.duckdb"

source = sql_database(credentials=f"sqlite:///{SQLITE_PATH}", schema="main")

pipeline = dlt.pipeline(
    pipeline_name="sakila_sqlite_to_duckdb",
    destination=dlt.destinations.duckdb(str(DUCKDB_PATH)),
    dataset_name="staging",
)

load_info = pipeline.run(source, write_disposition="replace")
print(load_info)
```


## Execute the ETL Load
| Command                                      | Expected Outcome                                                                |
| -------------------------------------------- | ------------------------------------------------------------------------------- |
|  `uv add "dlt[duckdb]"`                      |   Enables DuckDB as target destination                                          |
| `uv run python load_sakila_sqlite_duckdb.py` | Creates/refreshes `data/sakila.duckdb` containing tables under `staging` schema |

- After running the line above, sakila.duckdb is created.
- Connect: Create a jupyter ntbk called test_sakila.ipynb. Open that notebook. Change to your venv and to Python. In the ntbk, write

```sql
import duckdb

with duckdb.connect("data/sakila.duckdb") as conn:
    description = conn.sql("DESC").df()

description

```

## Connecting to DuckDB Correctly
Incorrect path:
```pgsql
duckdb.connect("19_data/sakila.duckdb")
```

DuckDB interprets this as:
```bash
19_dlt/19_dlt/data/sakila.duckdb
```
This path does not exist â†’ connection fails.

Correct path:
```pgsql
duckdb.connect("data/sakila.duckdb")
```

| Concept                    | Explanation                                               |
| -------------------------- | --------------------------------------------------------- |
| Notebook working directory | Always the folder where the `.ipynb` is saved (`19_dlt/`) |
| Database location          | `19_dlt/data/sakila.duckdb`                               |
| Required path              | Relative path: `data/sakila.duckdb`                       |


## Troubleshooting Summary
| Issue                                                    | Root Cause                                      | Fix                                                 |
| -------------------------------------------------------- | ----------------------------------------------- | --------------------------------------------------- |
| `sqlite3.OperationalError: unable to open database file` | Source SQLite file was empty or in wrong folder | Ensure `data/sqlite-sakila.db` exists and is > 5 MB |
| `IO Error: cannot open file ...19_dlt/19_dlt/data`       | Wrong path prefix in notebook                   | Use relative path: `"data/sakila.duckdb"`           |
| `CatalogException: Table actor does not exist`           | DLT loads into `staging` schema                 | Query using `staging.actor`                         |
