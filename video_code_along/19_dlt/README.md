# Workflow: Loading Sakila from SQLite into DuckDB Using DLT
```bash
19_dlt/                         # ETL workspace for loading Sakila SQLite → DuckDB using DLT
│
├── data/                       # Source + destination databases (all storage assets live here)
│   ├── sqlite-sakila.db        # Raw SQLite source from Kaggle (input for DLT pipeline)
│   └── sakila.duckdb           # DuckDB warehouse generated by the pipeline (staging schema)
│
├── load_sakila_sqlite_duckdb.py # DLT pipeline script: extracts from SQLite, loads into DuckDB
│                                # Configured with dataset_name="staging" and write_disposition="replace"
│
├── pyproject.toml              # Project metadata + dependency management created by uv
│                                # Defines project name, dependencies, and DLT/DuckDB integrations
│
├── README.md                   # Documentation describing workflow, setup, troubleshooting, and usage
│
└── test_sakila.ipynb           # Jupyter notebook for validation & EDA
                                 # Connects to data/sakila.duckdb and inspects loaded tables
``` 


## Terminal Commands
This end-to-end workflow provisions a local DuckDB warehouse from a SQLite source using dlt.
All steps below assume the working directory:
```bash
~/de25/aira_sql/video_code_along/19_dlt (main)
```

## Initialization
| Step                        | Command / Action                                 | Purpose                                      |
| --------------------------- | ------------------------------------------------ | -------------------------------------------- |
| 1. Create project structure | `uv init`                                        | Generates `pyproject.toml` and scaffolding   |
| 2. Rename project           | Update `name = "dlt_sakila"` in `pyproject.toml` | Standardizes package metadata                |
| 3. Add dependencies         | `uv add ipykernel "dlt[sql_database]"`           | Enables database extraction and notebook use |


## Preparing Data Assets
| File               | Location                       | Notes                                         |
| ------------------ | ------------------------------ | --------------------------------------------- |
| `sqlite-sakila.db` | `19_dlt/data/`                 | Downloaded from Kaggle; **must not be empty** |
| `sakila.duckdb`    | Auto-created in `19_dlt/data/` | Generated by DLT pipeline                     |

- Create a jupyterntbk called `load_sakila_sqlite_duckdb.py`

## Pipeline Script to write in `load_sakila_sqlite_duckdb.py`
```python
import dlt
from dlt.sources.sql_database import sql_database
from pathlib import Path

DATA_PATH = Path(__file__).parent / "data"
SQLITE_PATH = DATA_PATH / "sqlite-sakila.db"
DUCKDB_PATH = DATA_PATH / "sakila.duckdb"

source = sql_database(credentials=f"sqlite:///{SQLITE_PATH}", schema="main")

pipeline = dlt.pipeline(
    pipeline_name="sakila_sqlite_to_duckdb",
    destination=dlt.destinations.duckdb(str(DUCKDB_PATH)),
    dataset_name="staging",
)

load_info = pipeline.run(source, write_disposition="replace")
print(load_info)
```


## Execute the ETL Load
| Command                                      | Expected Outcome                                                                |
| -------------------------------------------- | ------------------------------------------------------------------------------- |
|  `uv add "dlt[duckdb]"`                      |   Enables DuckDB as target destination                                          |
| `uv run python load_sakila_sqlite_duckdb.py` | Creates/refreshes `data/sakila.duckdb` containing tables under `staging` schema |

- After running the line above, sakila.duckdb is created.
- Connect: Create a jupyter ntbk called `test_sakila` in ipynb. Open that notebook. Change to venv and to Python. 


## In the `test_sakila` ntbk, write:

```sql
import duckdb

with duckdb.connect("data/sakila.duckdb") as conn:
    description = conn.sql("DESC").df()

description
```

## Errors we fixed: Connecting to DuckDB Correctly
Incorrect path:
```pgsql
duckdb.connect("19_data/sakila.duckdb")
```

DuckDB interprets this as:
```bash
19_dlt/19_dlt/data/sakila.duckdb
```
This path does not exist → connection fails.

Correct path:
```pgsql
duckdb.connect("data/sakila.duckdb")
```

| Concept                    | Explanation                                               |
| -------------------------- | --------------------------------------------------------- |
| Notebook working directory | Always the folder where the `.ipynb` is saved (`19_dlt/`) |
| Database location          | `19_dlt/data/sakila.duckdb`                               |
| Required path              | Relative path: `data/sakila.duckdb`                       |


## Troubleshooting Summary
| Issue                                                    | Root Cause                                      | Fix                                                 |
| -------------------------------------------------------- | ----------------------------------------------- | --------------------------------------------------- |
| `sqlite3.OperationalError: unable to open database file` | Source SQLite file was empty or in wrong folder | Ensure `data/sqlite-sakila.db` exists and is > 5 MB |
| `IO Error: cannot open file ...19_dlt/19_dlt/data`       | Wrong path prefix in notebook                   | Use relative path: `"data/sakila.duckdb"`           |
| `CatalogException: Table actor does not exist`           | DLT loads into `staging` schema                 | Query using `staging.actor`                         |
